---
title: Classics List
description: Papers I keep coming back to
tags:
  - papers
  - classics
---

몇 번을 다시 봐도 배울 게 남는 글들. 이유를 짧게 붙여 둡니다.

- **LeNet-5: Gradient-Based Learning Applied to Document Recognition**
    
    Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner (1998)
    
    [Link (IEEE)](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)
    
    → CNN의 사실상 시작. 손글씨(MNIST) 인식에서 혁신적인 성능을 보여주며, 현대 컴퓨터 비전의 기초가 됨.
    
- **ImageNet Classification with Deep Convolutional Neural Networks (AlexNet)**
    
    Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton (2012)
    
    [arXiv:1404.5997](https://arxiv.org/abs/1404.5997)
    
    → GPU + ReLU + Dropout + 대규모 데이터셋을 통해 딥러닝 혁명을 촉발한 논문.
    
- **Very Deep Convolutional Networks for Large-Scale Image Recognition (VGGNet)**
    
    Karen Simonyan, Andrew Zisserman (2014)
    
    [arXiv:1409.1556](https://arxiv.org/abs/1409.1556)
    
    → 단순하지만 강력한 구조(3x3 컨볼루션 반복). 네트워크 깊이의 중요성을 보여줌.
    
- **Deep Residual Learning for Image Recognition (ResNet)**
    
    Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun (2015)
    
    [arXiv:1512.03385](https://arxiv.org/abs/1512.03385)
    
    → skip connection으로 깊은 신경망 학습을 가능하게 한 논문. 이후 대부분의 모델에 기본 구조가 됨.
    
- **Auto-Encoding Variational Bayes (VAE)**
    
    Kingma, Welling (2013)
    
    [arXiv:1312.6114](https://arxiv.org/abs/1312.6114)
    
    → 생성모델의 큰 축. 잠재 공간(latent space)에서 확률적 추론을 가능하게 함.
    
- **Generative Adversarial Nets (GAN)**
    
    Ian Goodfellow et al. (2014)
    
    [arXiv:1406.2661](https://arxiv.org/abs/1406.2661)
    
    → 생성모델의 또 다른 큰 축. 적대적 학습을 통한 이미지 생성 혁신.
    
- **Attention Is All You Need (Transformer)**
    
    Vaswani et al. (2017)
    
    [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)
    
    → RNN/CNN을 넘어 Attention 기반 구조만으로 SOTA 달성. 현재 LLM 시대를 연 논문.
