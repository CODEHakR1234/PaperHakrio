---
title: Classifier-Free Diffusion Guidance
year: 2022
description: Classifier 없이 Guidance를 확립할 수 있게 하는 기법 소개
arxiv: https://arxiv.org/pdf/2207.12598
github: https://github.com/lucidrains/classifier-free-guidance-pytorch?utm_source=chatgpt.com
tags:
  - CV
  - papers
  - "#SKKAI"
  - "#year-2025"
---

- 처음 제목을 읽었을 때는 classifier을 통해 diffusion sampling을 원하는 대로 보내는 사실 2번째 논문이지만 다음주에 읽는 그 논문이 생각 났다. 여기서는 그걸 넘어 classifier-free diffusion guidance를 시도하는 것 같았다.
- 논문을 순서대로 캡쳐하여 해당 부분을 읽고 든 생각을 작성하겠다.(영어로는 읽어도 잘 기억에 안 남는다. 한국어로 이해해야한다.)
- 나의 생각 <= 입니다

# Abstract 번역

분류기(Classifier) 가이던스는 최근 제안된 방법으로, 조건부 확산 모델에서 학습 후  
모드 커버리지(mode coverage)와 샘플 충실도(sample fidelity) 사이의 균형을 조절하는  
방법이다. 이는 다른 생성 모델에서의 저온 샘플링(low temperature sampling)이나  
절단(truncation)과 유사하다.

분류기 가이던스는 확산 모델의 점수 추정치(score estimate)와 이미지 분류기의  
그래디언트(gradient)를 결합하므로, 확산 모델과 별도로 분류기를 학습해야 한다.  
또한 “분류기 없이도 가이던스를 수행할 수 있는가?”라는 질문을 던진다.

저자들은 분류기 없이도 순수 생성 모델만으로 가이던스를 수행할 수 있음을 보인다.  
이를 **Classifier-Free Guidance**라 부르며, 조건부 확산 모델과 무조건적(unconditional)  
확산 모델을 함께 학습한 뒤, 두 점수 추정치를 결합하여 분류기 가이던스와 유사하게  
샘플 품질과 다양성 간의 trade-off를 달성한다.

- classifier라는 이야기를 처음에 시작하고 "조건부 확산 모델", "모드 커버리지", "샘플 충실도", "저온 샘플링", "절단" 놀랍게도 다 모르는 단어이다. 이걸 조금은 이해하고 넘어가야겠다.

> [!note]- 주요 단어 정의
**모드 커버리지 (Mode coverage)**
"모드"는 데이터 분포에서 자주 등장하는 패턴이나 범주를 뜻해요.
예를 들어, 고양이·강아지·새 이미지를 학습했다면, 각 동물 종류가 하나의 "모드"가 됩니다.
모드 커버리지는 모델이 모든 다양한 모드를 잘 커버하는지, 즉 다양한 패턴을 빠짐없이 생성하는 능력을 말합니다.  
>
**샘플 충실도 (Sample fidelity)**
생성된 데이터(예: 이미지)가 얼마나 진짜 같은지, 품질이 높은지를 나타내는 개념이에요.
충실도가 높으면 생성 이미지가 현실적인데, 대신 다양성이 줄어들 수도 있습니다.  
>
**저온 샘플링 (Low temperature sampling)**
샘플링할 때 확률 분포의 "온도(temperature)" 파라미터를 낮추면, 모델이 더 안전하고 평균적인 결과를 내요.
즉, 더 선명하지만 다양성은 줄어드는 효과가 있습니다.  
>
**절단 (Truncation)**
샘플링 과정에서 확률이 낮은 후보를 잘라내는 방법입니다.
흔히 GAN 같은 생성 모델에서 "품질을 높이기 위해 극단적이거나 이상한 샘플을 버리는 기법"이에요.  
>
**점수 추정치 (Score estimate)**
확산 모델은 데이터의 확률분포의 기울기(gradient), 즉 "점수(score)"를 학습합니다.
쉽게 말해, "노이즈가 낀 데이터를 원래 데이터 쪽으로 조금 더 움직일 방향"을 알려주는 벡터입니다.  
>
**분류기의 그래디언트 (Gradient from classifier)**
만약 우리가 조건부 이미지(예: "고양이")를 생성하고 싶다면, 분류기가 '이건 고양이일 확률'을 높이는 방향을 알려줍니다.
그 방향(gradient)을 점수 추정치에 더해주면, 원하는 조건에 맞는 이미지를 얻기 쉬워집니다.  
>
**분류기 가이던스 (Classifier guidance)**
확산 모델이 학습한 "노이즈 제거 방향(점수)"에, 분류기의 "조건 강화 방향"을 더해서 샘플 품질을 조정하는 방법입니다.
단점: 별도의 분류기 모델을 추가로 학습해야 한다는 점.  
>
**무조건적 모델 (Unconditional model)**
어떤 조건도 주지 않고 "그냥 데이터 분포 전체"를 학습한 모델입니다.
조건부 모델 (Conditional model)
특정 조건(예: "고양이" 라벨)을 주었을 때만 해당 데이터 분포를 학습하는 모델입니다.  
>
**분류기 없는 가이던스 (Classifier-Free Guidance, CFG)**
"분류기 없이도 가이던스가 가능할까?"라는 발상에서 나온 방법.
무조건적 모델과 조건부 모델을 하나의 모델에서 동시에 학습한 뒤, 두 점수 추정치를 선형 결합합니다.
결과적으로 별도의 분류기 없이도 샘플 품질 vs 다양성의 균형을 조절할 수 있습니다.  

**요약하면,**
분류기 가이던스: 확산 모델 + 추가 분류기 필요
분류기 없는 가이던스(CFG): 하나의 모델로 무조건/조건부 점수를 함께 학습 → 두 개를 섞어 쓰면 분류기 없이도 같은 효과

- 다시 초록을 읽어보자

분류기(Classifier) 가이던스는 최근 제안된 방법으로, 조건부 확산 모델에서 학습 후  
모드 커버리지(mode coverage)와 샘플 충실도(sample fidelity) 사이의 균형을 조절하는  
방법이다. 이는 다른 생성 모델에서의 저온 샘플링(low temperature sampling)이나  
절단(truncation)과 유사하다.

- 모드를 포괄하는 것과 샘플에 대한 퀄리티(다양성과 반비례 경향)의 균형이고 저온 샘플링, 절단 => 다양성을 줄어버림
- 결론적으로 조건부로 학습을 하다보니 전체적으로 모드를 포괄하지 못하지만 각 모드의 퀄리티는 좋아지겠네

분류기 가이던스는 확산 모델의 점수 추정치(score estimate)와 이미지 분류기의  
그래디언트(gradient)를 결합하므로, 확산 모델과 별도로 분류기를 학습해야 한다.  
또한 “분류기 없이도 가이던스를 수행할 수 있는가?”라는 질문을 던진다.

- 분류기가 확산 모델의 가이던스를 수행했는데 거기에 점수 추정치(score estimate)와 그래디언트(gradient)가 사용됬나보다.
- 이거 없이 가이던스를 수행할 수 있나? 신기하네

저자들은 분류기 없이도 순수 생성 모델만으로 가이던스를 수행할 수 있음을 보인다.  
이를 **Classifier-Free Guidance**라 부르며, 조건부 확산 모델과 무조건적(unconditional)  
확산 모델을 함께 학습한 뒤, 두 점수 추정치를 결합하여 분류기 가이던스와 유사하게  
샘플 품질과 다양성 간의 trade-off를 달성한다.

- 분류기가 필요없다 순수 생성 모델만으로 guidance를 수행할 수 있음을 보인다고 하네. 방법은 조건부 확산 모델과 무조건적 확산 모델을 함께 학습한 뒤, 두 점수 추정치를 결합하여 분류기 가이던스와 유사하게.. 이 둘을 섞으니깐 trade-off 달성하나보다. 근데 조건부 확산 모델 학습하는거를 모르는데 일단 더 읽어보자. 앞으로 번역 본 사이사이 나의 생각을 적겠다.

# 1.INTRODUCTION 번역

최근 확산 모델은 표현력이 뛰어나고 유연한 생성 모델군으로 주목받고 있으며,  
이미지·오디오 합성에서 경쟁력 있는 샘플 품질과 가능도(likelihood) 점수를 보여주고 있다  
(Sohl-Dickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020; Song et al., 2021b;  
Kingma et al., 2021; Song et al., 2021a).

- 그런가보다

이 모델들은 오디오 합성에서 훨씬 적은 추론 단계로도 자기회귀 모델에 버금가는 성능을  
달성하였으며 (Chen et al., 2021; Kong et al., 2021), ImageNet 생성에서는 BigGAN-deep  
(Brock et al., 2019)과 VQ-VAE-2 (Razavi et al., 2019)을 능가하는 FID 점수와 분류 정확도를  
달성하였다 (Ho et al., 2021; Dhariwal & Nichol, 2021).

- 대단하다

Dhariwal & Nichol (2021)은 추가로 학습된 분류기를 이용하여 확산 모델의 샘플 품질을  
향상시키는 기술인 **Classifier Guidance**를 제안하였다.

- 이거 설명하는 것 같다.

분류기 가이던스 이전에는, 확산 모델에서 Truncated BigGAN (Brock et al., 2019)이나  
저온 Glow (Kingma & Dhariwal, 2018)이 만들어내는 것과 유사한 “저온 샘플”을 생성하는  
방법이 알려져 있지 않았다.

- GAN에서 쓴 방법이 잘 안 먹힌 것 같다.

즉, 단순히 모델의 점수 벡터(score vectors)를 스케일링하거나, 확산 샘플링 과정에서 추가되는  
가우시안 노이즈의 양을 줄이는 방식은 효과적이지 않았다 (Dhariwal & Nichol, 2021).

이에 비해 분류기 가이던스는 확산 모델의 점수 추정치(score estimate)를  
분류기의 로그 확률(log probability)의 입력 그래디언트와 결합한다.

- 갑자기 어렵다. 추가 설명이 필요하다

> [!note]- 추가설명
> 
![](https://blog.kakaocdn.net/dna/MPDQN/btsQK9SljWJ/AAAAAAAAAAAAAAAAAAAAALe2Fv0JlC9gGoTU1WMk5dNtqZrfADUcaGn-HOYgQwSF/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1759244399&allow_ip=&allow_referer=&signature=E7SRzPXl4sa2AgLpsOBoU4BqOZ4%3D)

---

[그림 1] 64x64 ImageNet 확산 모델에서 malamute 클래스에 대해 분류기 없는 가이던스(Classifier-Free Guidance)를 적용한 예시.  
왼쪽은 비가이드 샘플(non-guided samples), 오른쪽으로 갈수록 가이던스 강도를 점차 높여가며 생성된 샘플을 보여준다.

- 오른쪽으로 갈 수록 개밖에 없다. 다양성 줄었지만 개 사진 퀄리티 업

[그림 2] 세 개의 가우시안 혼합 분포에 가이던스를 적용한 효과를 보여준다.  
각 혼합 구성 요소는 특정 클래스에 조건화된 데이터를 나타낸다.  
가장 왼쪽 그림은 비가이드된 주변 밀도(non-guided marginal density)이며,  
오른쪽으로 갈수록 가이던스 강도가 증가한 조건부 분포의 혼합 밀도를 보여준다.

- 가우시안 혼합 분포에 가이던스를 적용하니 분포가 분리된다.

분류기 그래디언트의 강도를 조절함으로써, Dhariwal & Nichol은  
Inception Score (Salimans et al., 2016)와 FID Score (Heusel et al., 2017)  
(즉, 정밀도와 재현율) 사이의 균형을 맞출 수 있음을 보였다.  
이는 BigGAN에서 truncation 파라미터를 조절하는 것과 유사하다.

저자들의 관심사는 **“분류기 없이도 가이던스를 수행할 수 있는가?”** 이다.  
분류기 가이던스는 추가 분류기 학습이 필요하기 때문에 훈련 파이프라인을 복잡하게 만든다.  
또한 이 분류기는 노이즈가 섞인 데이터로 훈련되어야 하므로,  
기존에 사전 학습된 분류기를 그대로 사용하는 것이 불가능하다.

더 나아가, 분류기 가이던스는 샘플링 중에 점수 추정치와 분류기 그래디언트를 결합하므로,  
이는 이미지 분류기를 교란시키려는 그래디언트 기반 적대적 공격(adversarial attack)으로  
해석될 수 있다. 따라서 “분류기 가이던스가 FID나 IS 같은 분류기 기반 지표를 높이는 것은  
단순히 분류기를 속였기 때문이 아닌가?”라는 의문이 생긴다.

또한 분류기 그래디언트를 따르는 것은 비매개적(nonparametric) 생성기를 사용하는 GAN 학습과도  
유사한데, 이는 곧 분류기 가이던스를 적용한 확산 모델이 이미 분류기 기반 지표에서 잘 알려진  
GAN의 특성을 닮아가고 있기 때문일 수도 있다.

이러한 의문을 해결하기 위해 저자들은 **Classifier-Free Guidance**를 제안한다.  
이 방법은 분류기를 전혀 사용하지 않고, 이미지 분류기의 그래디언트 방향으로 가는 대신,  
조건부 확산 모델과 동시에 학습된 무조건부 확산 모델의 점수 추정치를 결합한다.

혼합 가중치(mixing weight)를 조정함으로써, FID/IS 간의 trade-off를  
분류기 가이던스와 유사하게 달성할 수 있다.

실험 결과, classifier-free guidance는 순수 생성 확산 모델만으로도  
다른 생성 모델들과 맞먹는 고충실도(high-fidelity) 샘플을 합성할 수 있음을 보여준다.



